

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>4. Human Perception and Visual Encoding &#8212; Data Visualization - From a Human-Centered Perspective (Lecture Notes)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '04-perception';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="&lt;no title&gt;" href="05-tabulardata.html" />
    <link rel="prev" title="3. Understanding your Data" href="03-understanding.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    <p class="title logo__title">Data Visualization - From a Human-Centered Perspective (Lecture Notes)</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Data Visualization - From a Human-Centered Perspective (Lecture Notes)
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-intro.html">1. The Value of Data Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-process.html">2. The Process of Visualizing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-understanding.html">3. Understanding your Data</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. Human Perception and Visual Encoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">5. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F04-perception.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/04-perception.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Human Perception and Visual Encoding</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-perception">4.1. Human Perception</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-human-eye">4.1.1. The Human Eye</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#color-vision">4.1.2. Color Vision</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spation-vision">4.1.3. Spation Vision</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-attentive-processing">4.2. Pre-Attentive Processing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choice-of-encoding-bertins-guidance">4.2.1. Choice of Encoding - Bertin’s Guidance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-channels">4.3. Combining Channels</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="human-perception-and-visual-encoding">
<h1><span class="section-number">4. </span>Human Perception and Visual Encoding<a class="headerlink" href="#human-perception-and-visual-encoding" title="Permalink to this heading">#</a></h1>
<p>As we have discussed on the previous chapters, an important first step in data visualization is to contextualize your data. The next step is to select - based on the data characteristics - an effective visual encoding. Such visual encoding maps the data values to graphical features such as position, size, shape, and color. However, such mapping is not as straightforward as one might assume. A prerequisite is to understand how we, as humans, perceive our world, or more specifically visualizations. Ware states in his book <span id="id1">[<a class="reference internal" href="bibliography.html#id8" title="Colin Ware. Information visualization: perception for design. Morgan Kaufmann, 2019.">Ware, 2019</a>]</span>: “Understanding human perception can significantly improve both the quality and the quantity of information being displayed.” Such understanding allows us to design visualizations that can replace demanding cognitive calculations with simple perceptual inferences which often improve interpretability &amp; comprehension and, therefore, decision making <span id="id2">[<a class="reference internal" href="bibliography.html#id9" title="Jeffrey Heer, Michael Bostock, and Vadim Ogievetsky. A tour through the visualization zoo. Communications of the ACM, 53(6):59–67, Jun 2010. doi:10.1145/1743546.1743567.">Heer <em>et al.</em>, 2010</a>]</span>.</p>
<section id="human-perception">
<h2><span class="section-number">4.1. </span>Human Perception<a class="headerlink" href="#human-perception" title="Permalink to this heading">#</a></h2>
<p>Human perception is the ability to perceive our surroundings through the light that enters the eyes. The eye convert light into a series of electrochemical signals that are transmitted to the brain. This process can take as little as 13 milliseconds, according to a 2017 study by MIT in the United States <span id="id3">[<a class="reference internal" href="bibliography.html#id30" title="Tamara Munzner. Visualization analysis and design. CRC press, 2014.">Munzner, 2014</a>]</span>. However, human vision has a number of physical and perceptual limitations (concerning colors, patterns, and structures), which we should be aware of in order to create more effective data visualizations.</p>
<p>We can roughly divide perception into two stages <span id="id4">[<a class="reference internal" href="bibliography.html#id21" title="Stephen Few. Show me the numbers: designing tables and graphs to enlighten. Analytics Press, 2004. ISBN 978-0-9706019-9-5.">Few, 2004</a>]</span>. The sensations is the physical reception of the stimulus from the outside world, and the perception is a cognitive process that relates to the processing and interpretation of that stimulus. On the one hand the physical properties of the eye and the visual system mean that there are certain things that cannot be seen by the human; on the other hand, the interpretative capabilities of visual processing allow images to be constructed from incomplete information.</p>
<p>We need to understand both stages as both influence what can and cannot be perceived visually by a human being, which in turn directly affects the way that we design visualizations.</p>
<p>We begin with looking at the eye as a physical receptor, and then go on to consider the processing involved in basic vision.</p>
<section id="the-human-eye">
<h3><span class="section-number">4.1.1. </span>The Human Eye<a class="headerlink" href="#the-human-eye" title="Permalink to this heading">#</a></h3>
<p>Vision begins with light which is reflected from objects in the world. This light catches the eye and the image of these objects is projected upside down on the back of the eye, on the retina (this you might remember from school). In the following, we look into these components in more detail (cp. <a class="reference internal" href="#human-eye"><span class="std std-numref">Fig. 4.1</span></a>).</p>
<!--https://upload.wikimedia.org/wikipedia/commons/f/f5/Human_eye_diagram-sagittal_view-NEI.jpg -->
<figure class="align-center" id="human-eye">
<a class="reference internal image-reference" href="_images/humaneye.jpg"><img alt="_images/humaneye.jpg" src="_images/humaneye.jpg" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.1 </span><span class="caption-text">Components of the Human Eye, National Eye Institute. Taken from Wikipedia Commons. <a class="reference external" href="https://upload.wikimedia.org/wikipedia/commons/f/f5/Human_eye_diagram-sagittal_view-NEI.jpg">https://upload.wikimedia.org/wikipedia/commons/f/f5/Human_eye_diagram-sagittal_view-NEI.jpg</a></span><a class="headerlink" href="#human-eye" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>At the front of the eye are the cornea and lens which focus the light into a sharp image on the retina. The retina is the the light-sensitive layer of the eye. In a person with normal vision, the lens focuses images perfectly on a small depression in the back of the eye called the fovea (in German Sehgrube), which is part of the retina.</p>
<p>The retina contains two types of photoreceptor (light-sensitive cells): rods for low-light vision and cones for color vision.</p>
<p>Rods are highly sensitive to light and therefore allow us to see under a low level of lightning. However, they are unable to resolve fine detail and are subject to light saturation. This is the reason for the temporary blindness we get when moving from a darkened room into sunlight: the rods have been active and are saturated by the sudden light. As you can imagine, nowadays in our industrialized world, rods are hardly used <span id="id5">[<a class="reference internal" href="bibliography.html#id24" title="Jeff Johnson. Designing with the mind in mind: simple guide to understanding user interface design guidelines. Elsevier, Morgan Kaufmann is an imprint of Elsevier, second edition edition, 2014. ISBN 978-0-12-407914-4.">Johnson, 2014</a>]</span>. There are approximately 120 million rods per eye which are mainly situated towards the edges of the retina. Rods therefore dominate peripheral vision.</p>
<p>The cones are specialized types of photoreceptors that work best in bright light conditions. Cones are very sensitive to acute detail and provide tremendous spatial resolution. They are also directly involved in our ability to perceive color. The eye has approximately 6 million cones, mainly concentrated on the fovea. We can differentiate three types of cones. Each type of cone is sensitive to a range of light frequencies, and these sensitivity ranges overlap considerably <span id="id6">[<a class="reference internal" href="bibliography.html#id24" title="Jeff Johnson. Designing with the mind in mind: simple guide to understanding user interface design guidelines. Elsevier, Morgan Kaufmann is an imprint of Elsevier, second edition edition, 2014. ISBN 978-0-12-407914-4.">Johnson, 2014</a>]</span>.</p>
<!-- - Low frequency: these cones are sensitive to light over almost the entire range of visible light, but are most sensitive to the middle (yellow) and low (red) frequencies.
- Medium frequency: these cones respond to light ranging from the high-frequency blues through the lower middle-frequency yellows and oranges. Over- all, they are less sensitive than the low-frequency cones.
- High frequency: these cones are most sensitive to light at the upper end of the visible light spectrum—violets and blues—but they also respond weakly to middle frequencies, such as green. These cones are much less sensitive overall than the other two types of cones, and also less numerous. One result is that our eyes are much less sensitive to blues and violets than to other colors. 

Given the odd relationships among the sensitivities of our three types of retinal cone cells, one might wonder how the brain combines the signals from the cones to allow us to see a broad range of colors.
The answer is by subtraction. Neurons in the visual cortex at the back of our brain subtract the signals coming over the optic nerves from the medium- and low- frequency cones, producing a red–green difference signal channel. Other neurons in the visual cortex subtract the signals from the high- and low-frequency cones, yielding a yellow–blue difference signal channel. A third group of neurons in the visual cortex adds the signals coming from the low- and medium-frequency cones to produce an overall luminance (or black–white) signal channel.2 These three channels are called color-opponent channels.
The brain then applies additional subtractive processes to all three color-opponent channels: signals coming from a given area of the retina are effectively subtracted from similar signals coming from nearby areas of the retina.-->
<p>Although the retina is mainly covered with photoreceptors there is one blind spot where the optic nerve enters the eye. The blind spot has no rods or cones, yet our visual system compensates for this so that in normal circumstances we are unaware of it.</p>
<p>The retina also has specialized nerve cells called ganglion cells. There are two types: X-cells, which are concentrated in the fovea and are responsible for the early detection of pattern; and Y-cells which are more widely distributed in the retina and are responsible for the early detection of movement. The distribution of these cells means that, while we may not be able to detect changes in pattern in peripheral vision, we can perceive movement.</p>
</section>
<section id="color-vision">
<h3><span class="section-number">4.1.2. </span>Color Vision<a class="headerlink" href="#color-vision" title="Permalink to this heading">#</a></h3>
<p>We can describe color by different color space presentation. The RGB is the most popular one. However, in 1970, computer graphics researchers developed a model that closely aligns with the way human vision perceives color: the HSL (for hue, saturation, lightness) model. Hue is described in units of degree starting from 0 (red) to 360º.These are the typical colors you know. Saturation measures the degree to which a particular hue fully exhibits its essence. It goes from 0 % (grey) to 100% (very colorful). Lightness (or brightness) measures the degree to which color appears dark or light ranging from 0% (black) to fully 100% (white). To convert your colors, you can use webpages such as <a class="reference external" href="https://convertacolor.com/">ConvertColor</a> or libraries such as ‘colorspace’ in GNU R <span id="id7">[<a class="reference internal" href="bibliography.html#id22" title="Achim Zeileis, Jason C. Fisher, Kurt Hornik, Ross Ihaka, Claire D. McWhite, Paul Murrell, Reto Stauffer, and Claus O. Wilke. Colorspace: a toolbox for manipulating and assessing colors and palettes. arXiv:1903.06490 [cs, stat], Mar 2019. URL: http://arxiv.org/abs/1903.06490.">Zeileis <em>et al.</em>, 2019</a>]</span>. All HCL-based color palettes are also provided as discrete, continuous, and binned color scales for the use with the ggplot2 package <span id="id8">[<a class="reference internal" href="bibliography.html#id23" title="Hadley Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. ISBN 978-3-319-24277-4. URL: https://ggplot2.tidyverse.org.">Wickham, 2016</a>]</span>.</p>
<!-- python colorspace package: https://retostauffer.github.io/python-colorspace/articles/installation.html --><p>However, our color vision is limited <span id="id9">[<a class="reference internal" href="bibliography.html#id24" title="Jeff Johnson. Designing with the mind in mind: simple guide to understanding user interface design guidelines. Elsevier, Morgan Kaufmann is an imprint of Elsevier, second edition edition, 2014. ISBN 978-0-12-407914-4.">Johnson, 2014</a>]</span>. Our visual system is much more sensitive to differences in color and brightness, i.e., to contrasting colors and edges, than to absolute brightness levels. We perceive colors not in absolute terms (see three types of cones) but as the difference between the color we are focusing on and the color that surrounds it <span id="id10">[<a class="reference internal" href="bibliography.html#id25" title="Stephen Few. Now You See It: An Introduction to Visual Data Sensemaking. Analytics Press, first edition edition, 2009. ISBN 978-1-938377-12-9.">Few, 2009</a>]</span>. Our vision is therefore heavily impacted by the context.</p>
<p>There are three factors that affect our ability to distinguish colors from each other <span id="id11">[<a class="reference internal" href="bibliography.html#id24" title="Jeff Johnson. Designing with the mind in mind: simple guide to understanding user interface design guidelines. Elsevier, Morgan Kaufmann is an imprint of Elsevier, second edition edition, 2014. ISBN 978-0-12-407914-4.">Johnson, 2014</a>]</span>. First, the paler (less saturated) two colors are, the harder it is to tell them apart. Second, the the smaller or thinner objects are, the harder it is to distinguish their colors. Text is often thin, so the exact color of text is often hard to determine. Finally, the more separated color patches are, the more difficult it is to distinguish their colors, especially if the separation is great enough to require eye motion between patches.</p>
<p>But there are also external factors that influence how we can distinguish colors. Those factors are among others: the variation among color displays, gray-scale displays, display angle, and ambient lighting. These external factors are usually beyond your control but you should keep them in mind. Colors that appear distinguishable on your screen may not be so distinguishable in some of the environments in which the software is used.</p>
<!-- https://thenode.biologists.com/data-visualization-with-flying-colors/research/ -->
<p>A last issue you should consider is color blindness. Actual this term is mislabeled. It is not blindness, but rather a lack of color vision. It is the inability (or sometimes diminished ability) to see certain colors or perceive color contrasts in normal light. Color blindness is usually genetic, but in some cases it can be caused by disease or age. Furthermore, one in 12 men is color blind, compared to one in 200 women. The most common form of color blindness is red/green color blindness (protanopia). There are other, less common forms of color blindness that also involve different color pairs but also rare forms where colors can not be distinguished at all (achromatopsia).</p>
<p>To avoid that people cannot interpret your data visualizations correctly, you should use services such as <a class="reference external" href="https://colorbrewer2.org">ColorBrewer</a>. It supports you to select an appropriate color scheme by considering various user-selected criteria, including colorblind-friendliness.</p>
</section>
<section id="spation-vision">
<h3><span class="section-number">4.1.3. </span>Spation Vision<a class="headerlink" href="#spation-vision" title="Permalink to this heading">#</a></h3>
<p>The spatial resolution of the human visual field drops greatly from the center to the edges. In the center 1% of your visual field, i.e., the fovea, you have a high-resolution TIFF, and everywhere else, you have only a low-resolution JPEG. There are three reasons for this <span id="id12">[<a class="reference internal" href="bibliography.html#id24" title="Jeff Johnson. Designing with the mind in mind: simple guide to understanding user interface design guidelines. Elsevier, Morgan Kaufmann is an imprint of Elsevier, second edition edition, 2014. ISBN 978-0-12-407914-4.">Johnson, 2014</a>]</span>:</p>
<ul class="simple">
<li><p>Pixel density. Each eye has 6 to 7 million retinal cone cells. They are packed much more tightly in the fovea. The fovea has about 158,000 cone cells in each square millimeter. The rest of the retina has only 9,000 cone cells per square millimeter.</p></li>
<li><p>Data compression. Cone cells in the fovea connect 1:1 to the ganglial neuron cells that begin the processing and transmission of visual data, while elsewhere on the retina, multiple photoreceptor cells (cones and rods) connect to each ganglion cell. In technical terms, information from the visual periphery is compressed (with data loss) before transmission to the brain, while information from the fovea is not.</p></li>
<li><p>Processing resources. The fovea is only about 1% of the retina, but the brain’s visual cortex devotes about 50% of its area to input from the fovea. The other half of the visual cortex processes data from the remaining 99% of the retina.</p></li>
</ul>
<p>The result is that our vision has much, much greater resolution in the center of our visual field than elsewhere. If our peripheral vision has such low resolution, why do we see our surroundings sharply and clearly?</p>
<p>We experience this illusion because our eyes move rapidly and constantly about three times per second even when we don’t realize it, focusing our fovea on selected pieces of our environment. Our brain fills in the rest in a gross, impressionistic way based on what we know and expect. Our brain does not have to maintain a high-resolution mental model of our environment because it can order the eyes to sample and resample details in the environment as needed. We need this insights when we talk in the Chapter Interaction (Section &#64;ref(sec:interaction)).</p>
<!-- https://www.interaction-design.org/literature/article/preattentive-visual-properties-and-how-to-use-them-in-information-visualization -->
</section>
</section>
<section id="pre-attentive-processing">
<h2><span class="section-number">4.2. </span>Pre-Attentive Processing<a class="headerlink" href="#pre-attentive-processing" title="Permalink to this heading">#</a></h2>
<p>Many visual channels provide pre-attentive processing, where a distinct item stands out from many others immediately. The great value of preattentive processing is that the time it takes us to detect the different object does not depend on the number of distractor objects. Our low-level visual system performs massive parallel processing on these visual channels without requiring the viewer to consciously pay direct attention to the individual elements. Pre-attentive processing occurs for many channels. Examples are tilt, size, shape, proximity, and even shadow direction, but also various types of motion such as flicker, motion direction, and motion speed. However, a small number of potential channels do not support pre-attentive processing. One example is parallelism. Most visual channel pairs do not support pre-attentive processing, but some pairs do: one example is space and hue, and another is motion and shape. Pre-attentive processing is definitely not possible with three or more channels.</p>
<p>A question is, how we can systematically approach pre-attentive processing. Two principles can guide the use of visual channels in visual encoding: expressiveness and effectiveness. A set of facts that is <em>expressible</em> in a visual language if the sentences (i.e. the visualizations) in the language express all the facts in the set of data, and only the facts in the data. A visualization is more <em>effective</em> than another visualization if the information conveyed by one visualization is more readily perceived than the information in the other visualization.</p>
<p>The expressiveness principle dictates that the visual encoding should express all of, and only, the information that the dataset exhibits. For example, ordered data (nominal) should be shown in a way that our perceptual system intrinsically senses as ordered and unordered data (ordinal) should not be shown in a way that perceptually implies an ordering. The expressiveness principle is especially important, wenn we talk about ethics in data visualization (Section &#64;ref(sec:ethics))</p>
<p>The effectiveness principle dictates that the importance of the attribute should match the salience of the channel, i.e. its noticeability. We already talked about colors but there are more visual channels we can consider when we decide about the visual encoding. The first researcher who thought about the effectiveness of visualizations is <a class="reference external" href="https://en.wikipedia.org/wiki/Jacques_Bertin">Jacques Bertin</a>.</p>
<!-- https://graphworkflow.com/retinal/ -->
<!-- Check out: A Survey of Perception-Based Visualization Studies by Task -->
<section id="choice-of-encoding-bertins-guidance">
<h3><span class="section-number">4.2.1. </span>Choice of Encoding - Bertin’s Guidance<a class="headerlink" href="#choice-of-encoding-bertins-guidance" title="Permalink to this heading">#</a></h3>
<p>In 1967, Jacques Bertin published the book “Semiologie Graphique” (in English “Semiology of Graphics”) that was based on his long-standing experience as a cartographer and geographer. In this book, Bertin linked human perception to visualization. Even though, this linking was more based on intuition than vision research, Bertin’s experiences were later empirically proven. Bertin’s key concept is the image, which is is the fundamental perceptual unit of a visualization. An ideal visualizations will contain only a single image in order to optimize “efficiency,” the speed with which observer can extract the information.</p>
<p>Bertin identified in his work that every visualization is made by a series of basic components that have different expressive power and that each one works best only in some conditions. In general, the encoding of data can be done in a coordinate system with the cartesian coordinate system as its most prominent representative. Of course, there are further coordinate systems, such as geographical coordinate system, parallel coordinates system, polar coordinate system, or the network coordinate system. Based on the chosen coordinate system, you need to place your data, more precisely your data values (e.g., items, links) into these coordinates. For this, you can differentiate so-called <em>marks</em>. A mark is a basic graphical element which can be classified according to the number of spatial dimensions they require <span id="id13">[<a class="reference internal" href="bibliography.html#id30" title="Tamara Munzner. Visualization analysis and design. CRC press, 2014.">Munzner, 2014</a>]</span>: Possible dimensions are: a zero-dimensional (0D) mark is a point, a one-dimensional (1D) mark is a line, a two-dimensional (2D) mark is an area, a three- dimensional (3D) mark defines a volume.</p>
<figure class="align-center" id="visualattributes">
<a class="reference internal image-reference" href="_images/bertin_visualattributes.png"><img alt="_images/bertin_visualattributes.png" src="_images/bertin_visualattributes.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.2 </span><span class="caption-text">Bertin’s Visual Channels Taken from McDonald (1999).</span><a class="headerlink" href="#visualattributes" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>A visual channel defines the appearance of <em>marks</em> (points, lines, areas), independent its dimensionality. Bertin differentiated six visual channels: size, value, texture, color, orientation, shape. For each of these channels he pointed out in what cases they work best and how to use them (cp. Figure XX, redrawn from <span id="id14">[<a class="reference internal" href="bibliography.html#id27" title="L.W. MacDonald. Using color effectively in computer graphics. IEEE Computer Graphics and Applications, 19(4):20–35, Aug 1999. URL: http://ieeexplore.ieee.org/document/773961/, doi:10.1109/38.773961.">MacDonald, 1999</a>]</span>). We already discussed that the human eye is independently sensitive to these visual channels, which means that more than one visual channel can be deployed at the same time in order to encode different variation in the data.</p>
<!-- https://graphworkflow.com/retinal/ -->
<p>These visual channels can represent different relationships between marks:</p>
<ul class="simple">
<li><p>association (<span class="math notranslate nohighlight">\(\equiv\)</span>): the marks an be perceived as similar (<em>group</em>),</p></li>
<li><p>selection (<span class="math notranslate nohighlight">\(\ncong\)</span>): the marks are perceived as different, forming families (<em>distinguish</em>),</p></li>
<li><p>order (<span class="math notranslate nohighlight">\(O\)</span>): the marks are perceived as ordered (<em>sort</em>), and</p></li>
<li><p>quantity (<span class="math notranslate nohighlight">\(Q\)</span>): the marks are perceived as proportional to each other (<em>count</em>).</p></li>
</ul>
<p>These perceptional properties can be arranged into the levels of organization <span id="id15">[<a class="reference internal" href="bibliography.html#id26" title="Marc Green. Toward a perceptual science of multidimensional data visualization: bertin and beyond. ERGO/GERO Human Factors Science, 8:1–30, 1998.">Green, 1998</a>]</span>. Associative perception is the lowest level of organization. It allows grouping all elements of a variable in spite of different values. ‘’Selective perception’’ is the next higher level (flip side of association). It permits the viewer to select one category of a component, perceive locations of objects in that category and ignore others. Order allows the data to be ordinally ranked. An observer can see that one value of a variable represents a larger or smaller quantity than another. Quantity permits direct extraction of ratios, without need of consulting a legend, etc.</p>
<!-- Take examples from here: https://github.com/rseiter/ClevelandDataVis -->
<p>As said, Bertin’s research has been empirically substantiated. Most notable are perhaps Cleveland and McGill’s controlled experiments <span id="id16">[<a class="reference internal" href="bibliography.html#id44" title="William S. Cleveland and Robert McGill. Graphical perception: theory, experimentation, and application to the development of graphical methods. Journal of the American statistical association, 79(387):531–554, 1984.">Cleveland and McGill, 1984</a>]</span>. The most important findings is that they mapped human response directly to visually encoded abstract information and provide explicit rankings of perceptual accuracy for each channel type.</p>
<!-- Cleveland and McGill's experiments on magnitude channels showed that aligned position against a common scale is perceived most accurately, followed by unaligned position against an identical scale, followed by length, followed by angle. Area rankings are much less accurate than all others. They also suggest rankings for channels they have not directly tested: After area is an equivalence class of volume, curvature, and luminance; this class is followed by a hue in first place. (This last-place ranking is for hue as a magnitude channel, a very different matter from its second-place ranking as an identity channel). Cleveland and McGill's use their insights for redesigning existing graphs. -->
<p>Based on this work, Mackinlay <span id="id17">[<a class="reference internal" href="bibliography.html#id18" title="Jock Mackinlay. Automating the design of graphical presentations of relational information. ACM Transactions on Graphics, 5(2):110–141, Apr 1986. URL: https://doi.org/10.1145/22949.22950, doi:10.1145/22949.22950.">Mackinlay, 1986</a>]</span> has derived perceptually-motivated rankings of the effectiveness of variables such as position, length, area, and color for encoding quantitative data. Heer and Bostock <span id="id18">[<a class="reference internal" href="bibliography.html#id20" title="Jeffrey Heer and Michael Bostock. Crowdsourcing graphical perception: using mechanical turk to assess visualization design. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’10, 203–212. Association for Computing Machinery, Apr 2010. URL: https://doi.org/10.1145/1753326.1753357, doi:10.1145/1753326.1753357.">Heer and Bostock, 2010</a>]</span> confirmed and extended this work through crowdsourcing. The only discrepancy is that the later research found length and angle judgments that are roughly equivalent.</p>
<figure class="align-center" id="psychophysicalpowerlaw">
<a class="reference internal image-reference" href="_images/stevens_psychophysicalpowerlaw.png"><img alt="_images/stevens_psychophysicalpowerlaw.png" src="_images/stevens_psychophysicalpowerlaw.png" style="width: 50%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.3 </span><span class="caption-text">Results of Psychophysical power law of Stevens. Taken from Munzner (2014).</span><a class="headerlink" href="#psychophysicalpowerlaw" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Their results for visual encodings agree well with psychophysical channel measurements. Psychophysics is devoted to the systematic measurement of general human perception <span id="id19">[<a class="reference internal" href="bibliography.html#id43" title="Stanley Smith Stevens and Lawrence E. Marks. Psychophysics: Introduction to its perceptual, neural, and social prospects. Routledge, 2017.">Stevens and Marks, 2017</a>]</span>. We perceive different visual channels with different degrees of accuracy; they are not all equally distinguishable. Our responses to the sensory experience of size can be characterized by power-law/power-law, where the exponent depends on the precise sensory modality: Most stimuli are magnified or compressed; few remain unchanged. The diagram in Figure XX shows that length has an exponent of n = 1.0, so our perception of length is very close to the true value. Length means the length of a line segment on a 2D plane perpendicular to the observer. The other visual channels are not perceived as accurately: Area and brightness are compressed, while color saturation or electroshocks are magnified.</p>
</section>
</section>
<section id="combining-channels">
<h2><span class="section-number">4.3. </span>Combining Channels<a class="headerlink" href="#combining-channels" title="Permalink to this heading">#</a></h2>
<p>Multiple visual channels can be combined to redundantly encode the same attribute. The limitation of this approach is that more channels are ‘’consumed’’ so not as many attributes can be encoded in total, but the advantage is that the attributes represented are very well perceived.</p>
<p>However, visual channels are not completely independent of each other, since some have dependencies and interactions with others <span id="id20">[<a class="reference internal" href="bibliography.html#id30" title="Tamara Munzner. Visualization analysis and design. CRC press, 2014.">Munzner, 2014</a>]</span>. You must consider a continuum of potential interactions between channels for each pair, ranging from orthogonal and independent separable channels to inseparably combined integrated channels. Visual encoding is straightforward for single channels, but attempts to encode different information in integrated channels will fail. People will not be able to access the desired information about each attribute, but an unexpected combination will be perceived.</p>
<p><em>Separability of Visual Channels</em>: A pair of channels that are completely separable is position and hue.
An example of interference between the channels, showing that size is not completely separable from hue. Size interacts with many visual channels, including shape.
Integral pair is an encoding of one variable with horizontal size and another with vertical size. It is ineffective because what we perceive directly is the planar size of the circles, namely their area.
An inseparable pair of channels is the red and green channels of the RGB color space. These channels are not perceived separately, but are integrated into a combined color perception, so the three channels are not perceptual.</p>
<p><em>Grouping of Visual Channels</em>:
The encoding of link markers using containment/containment or link lines conveys the information that the linked objects form a group with a very strong perceptual cue. Containment is the strongest cue for grouping, with linkage a close second.
Another way to convey that elements form a group is to encode categorical data according to identity channels. All elements that share the same level of categorical attribute can be perceived as a group simply by selectively directing attention to that level. The perceptual grouping cue of identity channels is not as strong as using link or containment markers, but one advantage of this lightweight approach is that it does not add additional clutter in the form of extra link markers.
The third strongest grouping approach is proximity, which is the placement of objects within the same spatial region. This phenomenon of perceptual grouping is the reason why the best placed channel for encoding categorical data is a spatial region.
The final grouping channel is the similarity to the other categorical channels of hue and motion, and also shape if carefully selected. Logically, proximity is like similarity for spatial position; however, from a perceptual perspective, the effect of the spatial channels is so much stronger than the effect of the others that it makes sense to consider them separately.</p>
<!-- https://flylib.com/books/en/2.412.1/gestalt_principles_of_visual_perception.html --></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="03-understanding.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Understanding your Data</p>
      </div>
    </a>
    <a class="right-next"
       href="05-tabulardata.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">&lt;no title&gt;</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-perception">4.1. Human Perception</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-human-eye">4.1.1. The Human Eye</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#color-vision">4.1.2. Color Vision</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spation-vision">4.1.3. Spation Vision</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-attentive-processing">4.2. Pre-Attentive Processing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choice-of-encoding-bertins-guidance">4.2.1. Choice of Encoding - Bertin’s Guidance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-channels">4.3. Combining Channels</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Dr. Claudia Müller-Birn
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>